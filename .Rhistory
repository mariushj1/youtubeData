# Laver wordcloud
wordcloud2(wordcloud_df,
figPath = figPath,
size = 2,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
# Laver wordcloud
wordcloud2(wordcloud_df,
figPath = figPath,
size = 1,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
remove.packages("wordcloud2")
# Laver wordcloud
wordcloud2(wordcloud_df,
figPath = figPath,
size = 1,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
library(wordcloud2)
devtools::install_github("lchiffon/wordcloud2")
# Laver wordcloud
# Laver dataframe
wordcloud_df <- word_count %>%
select(word, n) %>%
top_n(150)
colnames(wordcloud_df) <- c("word", "freq")
# Indlæser figur
figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCgH-5Kph4tE_Q3SyKdo4vGQ/tea_shape.png"
# Laver farver
palette <- c("#e21a36", "#dbb2ae", "#f0f4fd", "#f0f4fd")
background_color <- "#15170c"
# Laver wordcloud
wordcloud2(wordcloud_df,
figPath = figPath,
size = 1,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
# Laver wordcloud
# Laver dataframe
wordcloud_df <- word_count %>%
select(word, n) %>%
top_n(500)
# Laver wordcloud
wordcloud2(wordcloud_df,
figPath = figPath,
size = 1,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 1,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
# Eksporterer
saveWidget(wordcloud, "wordcloud.html", selfcontained = FALSE)
webshot("wordcloud.html", "wordcloud.png", vwidth = 800, vheight = 600, zoom = 3)
webshot("wordcloud.html", "wordcloud.png", vwidth = 1500, vheight = 2000, zoom = 3)
webshot("wordcloud.html", "wordcloud.png", vwidth = 2000, vheight = 1500, zoom = 1)
webshot("wordcloud.html", "wordcloud.png", vwidth = 2000, vheight = 1400, zoom = 1)
# Indlæser figur
#figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCgH-5Kph4tE_Q3SyKdo4vGQ/tea_shape.png"
figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UC4xK7CfkqCbBcjXqpJlTIdw/weed_shape.jpeg"
# Kanal ID
channel_id <- "UC4xK7CfkqCbBcjXqpJlTIdw"
# Definerer vejen til mappen
folder_path <- paste0(channel_id,"/captions")
# Laver liste af alle .txt filer i mappen
txt_files <- list.files(folder_path, pattern = "\\.txt$", full.names = TRUE)
# Laver tom dataframe
captions_df <- tibble()
# Looper igennem hver fil
for (file in txt_files) {
# Læser teksten
text <- readLines(file)
# Læser navner
video_id <- basename(file)
# Laver ny række til dataframe
row <- data.frame(video_id, text)
# Binder rækken til dataframe
captions_df <- bind_rows(captions_df, row)
}
# Tæller ord
word_count <- captions_df %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
# Fjerner stopord
stopwords_manual <- data.frame(
word = c("uh", "hey", "yeah", "guys", "time","day","lot","gonna"),
lexicon = c("SMART", "SMART","SMART","SMART","SMART","SMART","SMART","SMART")
)
stopword_df <- rbind(stop_words, stopwords_manual)
word_count <- word_count %>%
anti_join(stopword_df, "word")
word_count %>% filter(word == "wheat")
word_count <- word_count %>%
anti_join(stopword_df, "word")
# Finder de længste ord
word_count$char_count <- nchar(word_count$word)
longest_words_df <- word_count[order(-word_count$char_count), ]
write.csv(word_count, file = paste0(channel_id,"/word_count.csv"), row.names = FALSE)
# Laver wordcloud
# Laver dataframe
wordcloud_df <- word_count %>%
select(word, n) %>%
top_n(500)
colnames(wordcloud_df) <- c("word", "freq")
# Indlæser figur
#figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCgH-5Kph4tE_Q3SyKdo4vGQ/tea_shape.png"
figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UC4xK7CfkqCbBcjXqpJlTIdw/weed_shape.jpeg"
palette <- c("#dab04a", "#cc8452", "#e3cdb8", "#025172", "#1b8bb0")
palette <- c("#dab04a", "#cc8452", "#e3cdb8", "#025172", "#1b8bb0")
background_color <- "#b3afae"
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 1,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
# Indlæser figur
#figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCgH-5Kph4tE_Q3SyKdo4vGQ/tea_shape.png"
figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UC4xK7CfkqCbBcjXqpJlTIdw/weed_shape.jpeg"
palette <- c("#dab04a", "#cc8452", "#e3cdb8", "#025172", "#1b8bb0")
background_color <- "#b3afae"
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 1,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
background_color <- "white"
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 1,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
palette <- c("#dab04a", "#cc8452", "#e3cdb8", "#025172", "#1b8bb0")
background_color <- "white"
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 1,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
background_color <- "green"
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 1,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 1,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
palette <- c("#dab04a", "#cc8452", "#e3cdb8", "#025172", "#1b8bb0")
background_color <- "white"
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 1,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
wordcloud_df
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 0.5,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 0.8,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
reticulate::repl_python()
# Kanal ID
channel_id <- "UCbvwGzSKXa31P6yDRyoNJnA"
# Definerer vejen til mappen
folder_path <- paste0(channel_id,"/captions")
# Laver liste af alle .txt filer i mappen
txt_files <- list.files(folder_path, pattern = "\\.txt$", full.names = TRUE)
# Laver tom dataframe
captions_df <- tibble()
# Looper igennem hver fil
for (file in txt_files) {
# Læser teksten
text <- readLines(file)
# Læser navner
video_id <- basename(file)
# Laver ny række til dataframe
row <- data.frame(video_id, text)
# Binder rækken til dataframe
captions_df <- bind_rows(captions_df, row)
}
# Tæller ord
word_count <- captions_df %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
# Fjerner stopord
stopwords_manual <- data.frame(
word = c("uh", "hey", "yeah", "guys", "time","day","lot","gonna"),
lexicon = c("SMART", "SMART","SMART","SMART","SMART","SMART","SMART","SMART")
)
stopword_df <- rbind(stop_words, stopwords_manual)
word_count <- word_count %>%
anti_join(stopword_df, "word")
# Finder de længste ord
word_count$char_count <- nchar(word_count$word)
longest_words_df <- word_count[order(-word_count$char_count), ]
longest_words_df
longest_words_df <- word_count[order(-word_count$char_count), ]
write.csv(word_count, file = paste0(channel_id,"/word_count.csv"), row.names = FALSE)
# Indlæser figur
#figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCgH-5Kph4tE_Q3SyKdo4vGQ/tea_shape.png"
#figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UC4xK7CfkqCbBcjXqpJlTIdw/weed_shape.jpeg"
figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCbvwGzSKXa31P6yDRyoNJnA/fancy_shape.jpg"
palette <- c("#4b94d6", "#b6c5dc", "#7dd0f0", "#2a547c", "#14366b", "#285da1", "#87afd1", "#2e704c", "#1b6db0")
background_color <- "#101310"
wordcloud_df
# Finder de længste ord
word_count$char_count <- nchar(word_count$word)
longest_words_df <- word_count[order(-word_count$char_count), ]
write.csv(word_count, file = paste0(channel_id,"/word_count.csv"), row.names = FALSE)
# Laver wordcloud
# Laver dataframe
wordcloud_df <- word_count %>%
select(word, n) %>%
top_n(500)
wordcloud_df
# Fjerner stopord
stopwords_manual <- data.frame(
word = c("uh", "hey", "yeah", "guys", "time","day","lot","gonna", "god"),
lexicon = c("SMART", "SMART","SMART","SMART","SMART","SMART","SMART","SMART", "SMART")
)
stopword_df <- rbind(stop_words, stopwords_manual)
word_count <- word_count %>%
anti_join(stopword_df, "word")
# Finder de længste ord
word_count$char_count <- nchar(word_count$word)
longest_words_df <- word_count[order(-word_count$char_count), ]
write.csv(word_count, file = paste0(channel_id,"/word_count.csv"), row.names = FALSE)
# Laver wordcloud
# Laver dataframe
wordcloud_df <- word_count %>%
select(word, n) %>%
top_n(500)
wordcloud_df
# Indlæser figur
#figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCgH-5Kph4tE_Q3SyKdo4vGQ/tea_shape.png"
#figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UC4xK7CfkqCbBcjXqpJlTIdw/weed_shape.jpeg"
figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCbvwGzSKXa31P6yDRyoNJnA/fancy_shape.jpg"
palette <- c("#4b94d6", "#b6c5dc", "#7dd0f0", "#2a547c", "#14366b", "#285da1", "#87afd1", "#2e704c", "#1b6db0")
background_color <- "#101310"
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 0.8,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 0.8,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
wordcloud_df
# Indlæser figur
#figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCgH-5Kph4tE_Q3SyKdo4vGQ/tea_shape.png"
#figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UC4xK7CfkqCbBcjXqpJlTIdw/weed_shape.jpeg"
figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCbvwGzSKXa31P6yDRyoNJnA/fancy_shape.jpg"
palette <- c("#4b94d6", "#b6c5dc", "#7dd0f0", "#2a547c", "#14366b", "#285da1", "#87afd1", "#2e704c", "#1b6db0")
background_color <- "#101310"
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 1,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 0.7,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
palette <- c("#4b94d6", "#b6c5dc", "#7dd0f0", "#2a547c", "#14366b", "#285da1", "#87afd1", "#2e704c", "#1b6db0")
background_color <- "white"
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 0.7,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
# Laver wordcloud
# Laver dataframe
wordcloud_df <- word_count %>%
select(word, n) %>%
top_n(100)
colnames(wordcloud_df) <- c("word", "freq")
# Indlæser figur
#figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCgH-5Kph4tE_Q3SyKdo4vGQ/tea_shape.png"
#figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UC4xK7CfkqCbBcjXqpJlTIdw/weed_shape.jpeg"
figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCbvwGzSKXa31P6yDRyoNJnA/fancy_shape.jpg"
palette <- c("#4b94d6", "#b6c5dc", "#7dd0f0", "#2a547c", "#14366b", "#285da1", "#87afd1", "#2e704c", "#1b6db0")
background_color <- "white"
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 0.7,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
wordcloud_df
# Laver wordcloud
# Laver dataframe
wordcloud_df <- word_count %>%
select(word, n) %>%
top_n(500)
colnames(wordcloud_df) <- c("word", "freq")
# Indlæser figur
#figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCgH-5Kph4tE_Q3SyKdo4vGQ/tea_shape.png"
#figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UC4xK7CfkqCbBcjXqpJlTIdw/weed_shape.jpeg"
figPath = "/Users/marius/Documents/DataAnalyseProjekter/YoutubeCaptions/UCbvwGzSKXa31P6yDRyoNJnA/fancy_shape.jpg"
palette <- c("#4b94d6", "#b6c5dc", "#7dd0f0", "#2a547c", "#14366b", "#285da1", "#87afd1", "#2e704c", "#1b6db0")
background_color <- "white"
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 0.7,
color = rep_len(palette, nrow(wordcloud_df)),
backgroundColor = background_color,
fontFamily = "Verdana")
wordcloud
# Laver wordcloud
wordcloud <- wordcloud2(wordcloud_df,
figPath = figPath,
size = 0.7,
color = "white",
backgroundColor = "#1b6db0",
fontFamily = "Verdana")
wordcloud
captions_df
View(captions_df)
# Laver sentiment analyse på hver video
sentiment_data <- captions_df %>%
unnest_tokens(word, text) %>%
inner_join(get_sentiments("bing")) %>%
count(video_id, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment_score = positive - negative)
sentiment_data
# Print the sentiment data
print(sentiment_data)
View(sentiment_data)
# Tæller ord
word_count <- captions_df %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
word_count
# Fjerner stopord
stopwords_manual <- data.frame(
word = c("uh", "hey", "yeah", "guys", "time","day","lot","gonna", "god"),
lexicon = c("SMART", "SMART","SMART","SMART","SMART","SMART","SMART","SMART", "SMART")
)
stopword_df <- rbind(stop_words, stopwords_manual)
word_count <- word_count %>%
anti_join(stopword_df, "word")
word_count
View(word_count)
library(tidyverse)
library(tidytext)
library(stopwords)
library(wordcloud2)
library(RColorBrewer)
library(htmlwidgets)
library(webshot)
# Kanal ID
channel_id <- "UCbvwGzSKXa31P6yDRyoNJnA"
# Definerer vejen til mappen
folder_path <- paste0("channels/",channel_id,"/captions")
# Kanal ID
channel_name <- "Harvest"
# Definerer vejen til mappen
folder_path <- paste0("channels/",channel_id,"/captions")
youtube_data <- read.csv(file = past0(folder_path, "/youtube_data.csv"))
youtube_data <- read.csv(file = paste0(folder_path, "/youtube_data.csv"))
# Definerer vejen til mappen
folder_path <- paste0("channels/",channel_name,"/captions")
youtube_data <- read.csv(file = paste0(folder_path, "/youtube_data.csv"))
# Definerer vejen til mappen
folder_path <- paste0("channels/",channel_name)
youtube_data <- read.csv(file = paste0(folder_path, "/youtube_data.csv"))
View(youtube_data)
youtube_data$content[1]
caption_df <- youtube_data %>%
select(title, content)
# Tæller ord
word_count <- captions_df %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
captions_df <- youtube_data %>%
select(title, content)
# Tæller ord
word_count <- captions_df %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
library(tidyverse)
library(tidytext)
library(stopwords)
library(wordcloud2)
library(RColorBrewer)
library(htmlwidgets)
library(webshot)
captions_df <- youtube_data %>%
select(title, content)
View(caption_df)
# Tæller ord
word_count <- captions_df %>%
unnest_tokens(word, text) %>%
count(word, sort = TRUE)
# Tæller ord
word_count <- captions_df %>%
unnest_tokens(word, content) %>%
count(word, sort = TRUE)
View(word_count)
# Kanal ID
channel_name <- "Harvest"
# Definerer vejen til mappen
folder_path <- paste0("channels/",channel_name)
youtube_data <- read.csv(file = paste0(folder_path, "/youtube_data.csv"))
# Tæller ord
word_count <- youtube_data %>%
unnest_tokens(word, content) %>%
count(word, sort = TRUE)
# Laver sentiment analyse på hver video
sentiment_data <- captions_df %>%
unnest_tokens(word, text) %>%
inner_join(get_sentiments("bing")) %>%
count(video_id, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment_score = positive - negative)
# Laver sentiment analyse på hver video
sentiment_data <- youtube_data %>%
unnest_tokens(word, text) %>%
inner_join(get_sentiments("bing")) %>%
count(video_id, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment_score = positive - negative)
# Laver sentiment analyse på hver video
sentiment_data <- youtube_data %>%
unnest_tokens(word, content) %>%
inner_join(get_sentiments("bing")) %>%
count(video_id, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment_score = positive - negative)
View(youtube_data)
# Laver sentiment analyse på hver video
sentiment_data <- youtube_data %>%
unnest_tokens(word, content) %>%
inner_join(get_sentiments("bing")) %>%
count(video_title, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment_score = positive - negative)
# Laver sentiment analyse på hver video
sentiment_data <- youtube_data %>%
unnest_tokens(word, content) %>%
inner_join(get_sentiments("bing")) %>%
count(title, sentiment) %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment_score = positive - negative)
sentiment_data
View(sentiment_data)
